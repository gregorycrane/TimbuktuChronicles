{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingSpacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvbZ0wIdH55g"
      },
      "source": [
        "# Training Spacey\n",
        "### Timbuktu Chronicles\n",
        "#### This file was created and used on Google Colab. The link to view the most updated version is: https://colab.research.google.com/drive/1H1Y8LUTKYfBvy8GUm8rOXLQL-AKVhieH?usp=sharing\n",
        "\n",
        "Some linkes regarding the training data:\n",
        "- https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
        "- https://www.machinelearningplus.com/training-custom-ner-model-in-spacy/ \n",
        "- https://www.youtube.com/watch?v=YBRF7tq1V-Q \n",
        "  - and the corresponding code file: https://github.com/wjbmattingly/ner_youtube/blob/main/lessons/04_02_customizing_spacy.py\n",
        "\n",
        "To-do:\n",
        "- See how to get the output of the spacey NER into a file so you can edit it \n",
        "(might have to do with the save_data function in the code for the video above - and also for the code in the lesson before it, 4.1)\n",
        "- Correct the spacey output\n",
        "- Use the corrected output to upload back to Spacey and use for training\n",
        "\n",
        "Refer to TimbuktuChronicles Google Colab notebook for some initial examples with Spacy and file access.\n",
        "\n",
        "Some segments of my code was based off of the YouTube video https://www.youtube.com/watch?v=YBRF7tq1V-Q , and corresponding code file: https://github.com/wjbmattingly/ner_youtube/blob/main/lessons/04_02_customizing_spacy.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsHlCYMH3kQ"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from pprint import pprint\n",
        "import json"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_N04wsJTqfq"
      },
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50DgHUsm0Xm-",
        "outputId": "c86d8794-7fa6-411e-ead8-f4e6db2c6e15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCReW40S0qJY"
      },
      "source": [
        "file1 = open(\"/content/gdrive/My Drive/fattashEnglish.txt\", \"r\")\n",
        "text = file1.read()\n",
        "#sample"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ZRgYX-0aPa"
      },
      "source": [
        "doc1 = nlp(text)\n",
        "#doc1\n",
        "#doc1.ents - all the entities in the doc\n",
        "# pprint([(X, X.ent_iob_, X.ent_type_) for X in doc1]) list like: [(In, 'O', ''),(Muhammad, 'B', 'PERSON'), etc]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMfL-LhG7AS1"
      },
      "source": [
        "def test_model(text):\n",
        "    doc = nlp(text)\n",
        "    results = []\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
        "    if len(entities) > 0:\n",
        "        results = [text, {\"entities\": entities}]\n",
        "        return (results)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYa_ZJUH7OA5"
      },
      "source": [
        "TRAIN_DATA = []\n",
        "file1 = open(\"/content/gdrive/My Drive/fattashEnglish.txt\", \"r\")\n",
        "\n",
        "for line in file1.readlines():\n",
        "  #print(line.strip())\n",
        "   line = line.strip()\n",
        "   results = test_model(line)\n",
        "\n",
        "   if results != None:\n",
        "     TRAIN_DATA.append(results)\n",
        "\n",
        "#TRAIN_DATA"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphffkIo-m8O"
      },
      "source": [
        "def save_data(file, data):\n",
        "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhgqtLg5-oqS"
      },
      "source": [
        "save_data(\"/content/gdrive/My Drive/fattash_training_data.json\", TRAIN_DATA)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDfmN60EA6MY"
      },
      "source": [
        "### To-do:\n",
        "- correct the output JSON file\n",
        "- train the model \n",
        "  - see part 5 here: https://www.machinelearningplus.com/training-custom-ner-model-in-spacy/ \n",
        "  - and https://github.com/wjbmattingly/ner_youtube/blob/main/lessons/04_03_customizing_spacy.py\n",
        "\n",
        "  Code for training the model is based off of the following 2 articles:\n",
        "  - https://www.machinelearningplus.com/training-custom-ner-model-in-spacy/\n",
        "  - https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFODX4cwRO2F"
      },
      "source": [
        "# load the training data back from the JSON file, via https://github.com/wjbmattingly/ner_youtube/blob/main/lessons/04_03_customizing_spacy.py\n",
        "def load_data(file):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return (data)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPb5F_9eRvdv"
      },
      "source": [
        "TRAIN_DATA_reloaded = load_data(\"/content/gdrive/My Drive/fattash_training_data.json\")\n",
        "#TRAIN_DATA_reloaded"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AkVyxp5SoyN"
      },
      "source": [
        "# training the model, via part 5 here:  https://www.machinelearningplus.com/training-custom-ner-model-in-spacy/\n",
        "\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(30):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "        print(\"Losses\", losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYZlHsxKUBCc",
        "outputId": "a0de48bb-3a07-46b1-a0c6-d4e55bbf06f7"
      },
      "source": [
        "# via https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7\n",
        "\n",
        "# load a spacy model\n",
        "nlp = spacy.blank('en')  \n",
        "\n",
        "# create ner pipeline:\n",
        "ner = nlp.create_pipe('ner')\n",
        "nlp.add_pipe(ner, last=True)\n",
        "\n",
        "# add the labels from TRAIN_DATA into the ner:\n",
        "for _, annotations in TRAIN_DATA:\n",
        "    for ent in annotations.get('entities'):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "#we only want to train NER, so this code tells it to ignore training everything else\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(30): # customize the number of iterations\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        for text, annotations in tqdm(TRAIN_DATA):\n",
        "            nlp.update(\n",
        "                [text],  \n",
        "                [annotations],  \n",
        "                drop=0.5,  \n",
        "                sgd=optimizer,\n",
        "                losses=losses)\n",
        "        print(losses) # not sure why I have this 100% - 5% pattern or why the numbers following 'ner' are so high. Maybe I should start with a different spacy model for nlp?"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.57it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 29.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 331.21830189854023}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.66it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 176.8498071495291}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.98it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 26.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 173.99772750839895}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.32it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 27.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 186.56557533294767}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.18it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 25.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 175.31104474629694}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.78it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 25.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 173.32067178879834}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.37it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 168.92811991217178}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.52it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 27.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 161.32138981455788}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.42it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 25.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 154.5503152997709}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.41it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 152.97940940494615}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.07it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 27.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 144.33112511701378}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.37it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 26.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 143.7167504663916}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.01it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 168.07334748170746}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.38it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 121.04581086331149}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.36it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 24.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 139.69297076766966}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.57it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 28.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 135.43982019765696}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.90it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 29.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 125.87490832533246}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.27it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 29.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 143.4997055427636}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.18it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 24.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 126.06918425688788}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 27.15it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 23.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 107.12106580237638}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.90it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 27.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 119.29065892582602}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.51it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 27.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 113.73083472794106}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 26.10it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 25.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 130.97255475501822}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 24.38it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 22.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 110.13793326073511}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 22.77it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 21.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 106.52992586787184}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 22.25it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 22.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 114.26969524454205}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 22.16it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 22.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 128.11296456400402}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 22.40it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 24.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 126.10233196985612}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 23.22it/s]\n",
            "  5%|▍         | 3/65 [00:00<00:02, 23.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 108.72646844654555}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 65/65 [00:02<00:00, 24.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'ner': 105.88652681170483}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmSHHr-LXBPz",
        "outputId": "28d2c110-57e0-4057-a86d-e42857504972"
      },
      "source": [
        "# test the trained model: \n",
        "for text, _ in TRAIN_DATA:\n",
        "    doc = nlp(text)\n",
        "    #print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# might need to split the TRAIN_DATA into TRAIN and TEST parts?\n",
        "\n",
        "# testing the model on something it hasn't seen before:\n",
        "doc = nlp(\"Askia Muhammad was the Khalifa of the Songhay Empire in the fifteenth/sixteenth century.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities [('Songhay', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUqmSwmXawJ"
      },
      "source": [
        "# saving/reloading the model so you don't have to train each time, via https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7\n",
        "\n",
        "# saving the model:\n",
        "\"\"\"\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)\n",
        "\"\"\"\n",
        "\n",
        "# reloading the model\n",
        "# model = spacy.load('model_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Y0sDeC02LW"
      },
      "source": [
        "# visuals\n",
        "# displacy.render(doc, jupyter=True, style='ent') - coloring the entities\n",
        "# displacy.render(doc, style='dep', jupyter = True, options = {'distance': 120}) - graph"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}